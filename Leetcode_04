Views

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| article_id    | int     |
| author_id     | int     |
| viewer_id     | int     |
| view_date     | date    |
+---------------+---------+
There is no primary key (column with unique values) for this table, the table may have duplicate rows.
Each row of this table indicates that some viewer viewed an article (written by some author) on some date. 
Note that equal author_id and viewer_id indicate the same person.
 

Write a solution to find all the authors that viewed at least one of their own articles.

Return the result table sorted by id in ascending order.

The result format is in the following example.

 

Example 1:

Input: 
Views table:
+------------+-----------+-----------+------------+
| article_id | author_id | viewer_id | view_date  |
+------------+-----------+-----------+------------+
| 1          | 3         | 5         | 2019-08-01 |
| 1          | 3         | 6         | 2019-08-02 |
| 2          | 7         | 7         | 2019-08-01 |
| 2          | 7         | 6         | 2019-08-02 |
| 4          | 7         | 1         | 2019-07-22 |
| 3          | 4         | 4         | 2019-07-21 |
| 3          | 4         | 4         | 2019-07-21 |
+------------+-----------+-----------+------------+
Output: 
+------+
| id   |
+------+
| 4    |
| 7    |
+------+


from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql.window import *
import pandas as pd



# Data for the Views table
data = [
    (1, 3, 5, "2019-08-01"),
    (1, 3, 6, "2019-08-02"),
    (2, 7, 7, "2019-08-01"),
    (2, 7, 6, "2019-08-02"),
    (4, 7, 1, "2019-07-22"),
    (3, 4, 4, "2019-07-21"),
    (3, 4, 4, "2019-07-21"),
]

# Define schema for the DataFrame
columns = ["article_id", "author_id", "viewer_id", "view_date"]

# Create the DataFrame
views_df = spark.createDataFrame(data, schema=columns)

# Show the DataFrame
views_df.show()
# Data for the Views table
data = [
    (1, 3, 5, "2019-08-01"),
    (1, 3, 6, "2019-08-02"),
    (2, 7, 7, "2019-08-01"),
    (2, 7, 6, "2019-08-02"),
    (4, 7, 1, "2019-07-22"),
    (3, 4, 4, "2019-07-21"),
    (3, 4, 4, "2019-07-21"),
]

# Define schema for the DataFrame
columns = ["article_id", "author_id", "viewer_id", "view_date"]

# Create the DataFrame
views_df = spark.createDataFrame(data, schema=columns)

# Show the DataFrame
views_df.show()
# Data for the Views table
data = [
    (1, 3, 5, "2019-08-01"),
    (1, 3, 6, "2019-08-02"),
    (2, 7, 7, "2019-08-01"),
    (2, 7, 6, "2019-08-02"),
    (4, 7, 1, "2019-07-22"),
    (3, 4, 4, "2019-07-21"),
    (3, 4, 4, "2019-07-21"),
]

# Define schema for the DataFrame
columns = ["article_id", "author_id", "viewer_id", "view_date"]

# Create the DataFrame
views_df = spark.createDataFrame(data, schema=columns)

# Show the DataFrame
views_df.show()


#pyspark
views_df.show()
new_df=views_df.select(col("author_id").alias("id")).filter((col("author_id"))==(col("viewer_id"))).distinct().show()

#spark_sql
views_df.createOrReplaceTempView("views_table")
final=spark.sql("select distinct author_id as id from views_table where author_id=viewer_id order by id").show()

#pandas
# Select the "author_id" column as "id" and filter rows where "author_id" == "viewer_id"
filtered_df = views_df[views_df["author_id"] == views_df["viewer_id"]][["author_id"]].rename(columns={"author_id": "id"}).drop_duplicates()

# Display the resulting DataFrame
print(filtered_df)
